{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "#task 1 \nimport torch\nimport torch.nn as nn\n\nclass MultiHeadAttention(nn.Module):\n    def _init_(self, d_model, n_heads):\n        super(MultiHeadAttention, self)._init_()\n        self.n_heads = n_heads\n        self.d_head = d_model // n_heads\n        self.linear_q = nn.Linear(d_model, d_model)\n        self.linear_k = nn.Linear(d_model, d_model)\n        self.linear_v = nn.Linear(d_model, d_model)\n        self.linear_out = nn.Linear(d_model, d_model)\n\n    def forward(self, q, k, v, mask=None):\n        q, k, v = self.linear_q(q), self.linear_k(k), self.linear_v(v)\n        q, k, v = self.split_heads(q), self.split_heads(k), self.split_heads(v)\n        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.d_head**0.5)\n\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, float('-inf'))\n\n        attn_weights = nn.functional.softmax(scores, dim=-1)\n        attn_output = torch.matmul(attn_weights, v)\n        attn_output = self.combine_heads(attn_output)\n        output = self.linear_out(attn_output)\n\n        return output\n\n    def split_heads(self, x):\n        batch_size, seq_len, d_model = x.size()\n        return x.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2).contiguous().view(batch_size * self.n_heads, seq_len, self.d_head)\n\n    def combine_heads(self, x):\n        batch_size, seq_len, _ = x.size()\n        return x.view(batch_size // self.n_heads, self.n_heads, seq_len, self.d_head).transpose(1, 2).contiguous().view(batch_size // self.n_heads, seq_len, -1)\n\nclass PositionalEncoding(nn.Module):\n    def _init_(self, d_model, max_len=512):\n        super(PositionalEncoding, self)._init_()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1).float()\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)].detach()\n\nclass FeedForward(nn.Module):\n    def _init_(self, d_model, d_ff, dropout=0.1):\n        super(FeedForward, self)._init_()\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(d_ff, d_model)\n\n    def forward(self, x):\n        x = nn.functional.relu(self.linear1(x))\n        x = self.dropout(x)\n        x = self.linear2(x)\n        return x\n\nclass TransformerBlock(nn.Module):\n    def _init_(self, d_model, n_heads, d_ff, dropout=0.1):\n        super(TransformerBlock, self)._init_()\n        self.attention = MultiHeadAttention(d_model, n_heads)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n        self.ff = FeedForward(d_model, d_ff)\n        self.norm2 = nn.LayerNorm(d_model)\n\n    def forward(self, x, mask=None):\n        x = x + self.dropout(self.attention(x, x, x, mask))\n        x = self.norm1(x)\n        x = x + self.dropout(self.ff(x))\n        x = self.norm2(x)\n        return x\n\nclass GPT2(nn.Module):\n    def _init_(self, vocab_size, d_model=768, n_heads=12, d_ff=3072, n_layers=12):\n        super(GPT2, self)._init_()\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        self.positional_encoding = PositionalEncoding(d_model)\n        self.transformer_blocks = nn.ModuleList([TransformerBlock(d_model, n_heads, d_ff) for _ in range(n_layers)])\n        self.fc = nn.Linear(d_model, vocab_size)\n\n    def forward(self, x, mask=None):\n        x = self.embedding(x)\n        x = self.positional_encoding(x)\n        for block in self.transformer_blocks:\n            x = block(x, mask)\n        x = self.fc(x)\n        return x",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "260c590b-b0d4-46bf-b5c5-12396745a7b7"
    }
  ]
}